{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2142cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time, datetime\n",
    "import matplotlib; matplotlib.use('Agg')\n",
    "from src import config, data\n",
    "from src.checkpoints import CheckpointIO\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb003eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "cfg = config.load_config('configs/pointcloud/grid.yaml', 'configs/default.yaml')\n",
    "is_cuda = (torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e1ea0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "# Shorthands\n",
    "out_dir = cfg['training']['out_dir']\n",
    "batch_size = cfg['training']['batch_size']\n",
    "backup_every = cfg['training']['backup_every']\n",
    "vis_n_outputs = cfg['generation']['vis_n_outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab4b41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_metric = cfg['training']['model_selection_metric']\n",
    "if cfg['training']['model_selection_mode'] == 'maximize':\n",
    "    model_selection_sign = 1\n",
    "elif cfg['training']['model_selection_mode'] == 'minimize':\n",
    "    model_selection_sign = -1\n",
    "else:\n",
    "    raise ValueError('model_selection_mode must be '\n",
    "                     'either maximize or minimize.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19dfcda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'points': array([[ 0.06367018, -0.23341139, -0.35384396],\n",
      "       [ 0.06717573,  0.40972158, -0.5414992 ],\n",
      "       [-0.48746377, -0.18413779, -0.45706937],\n",
      "       ...,\n",
      "       [ 0.2570478 ,  0.07432955,  0.41330966],\n",
      "       [-0.28128406, -0.3550816 , -0.46653956],\n",
      "       [ 0.38124532,  0.01783335, -0.27706045]], dtype=float32), 'points.occ': array([1., 0., 0., ..., 0., 0., 0.], dtype=float32), 'inputs': array([[ 0.12231445,  0.31811523,  0.04150391],\n",
      "       [ 0.23632812,  0.3215332 ,  0.32861328],\n",
      "       [ 0.11230469, -0.22375488, -0.37231445],\n",
      "       ...,\n",
      "       [ 0.1940918 ,  0.32080078, -0.21105957],\n",
      "       [-0.1538086 , -0.02494812,  0.37280273],\n",
      "       [ 0.07141113,  0.32006836,  0.3984375 ]], dtype=float32), 'inputs.normals': array([[-0.62597656, -0.7753906 ,  0.0836792 ],\n",
      "       [ 0.00189972, -1.        , -0.01567078],\n",
      "       [ 0.04974365, -0.15429688, -0.9868164 ],\n",
      "       ...,\n",
      "       [ 0.08111572, -0.99609375, -0.03234863],\n",
      "       [ 0.8754883 , -0.15881348,  0.45629883],\n",
      "       [ 0.0077095 , -0.9995117 ,  0.0241394 ]], dtype=float32)}\n",
      "<src.data.core.Shapes3dDataset object at 0x7f0cf7b00f60>\n",
      "2048\n",
      "5958\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "shutil.copyfile('configs/pointcloud/grid.yaml', os.path.join(out_dir, 'config.yaml'))\n",
    "\n",
    "# Dataset\n",
    "train_dataset = config.get_dataset('train', cfg)\n",
    "val_dataset = config.get_dataset('val', cfg, return_idx=True)\n",
    "print(train_dataset[5])\n",
    "print(train_dataset)\n",
    "print(len(train_dataset[5]['points.occ']))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42160475",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9d7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset[5], batch_size=batch_size, num_workers=cfg['training']['n_workers'], shuffle=True,\n",
    "#     collate_fn=data.collate_remove_none,\n",
    "#     worker_init_fn=data.worker_init_fn)\n",
    "\n",
    "# print(len(train_loader))    \n",
    "# print(train_loader)\n",
    "\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     val_dataset, batch_size=1, num_workers=cfg['training']['n_workers_val'], shuffle=False,\n",
    "#      collate_fn=data.collate_remove_none,\n",
    "#      worker_init_fn=data.worker_init_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60799253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amine code subset\n",
    "\n",
    "train_dataset = config.get_dataset(\"train\", cfg)\n",
    "ds = torch.utils.data.Subset(train_dataset, indices= [0]*len(train_dataset))\n",
    "val_dataset = config.get_dataset(\"val\", cfg)\n",
    "val_ds = torch.utils.data.Subset(val_dataset, indices= [0]*len(val_dataset))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    ds, batch_size=32, num_workers=8, shuffle=True,\n",
    "    collate_fn=data.collate_remove_none,\n",
    "    worker_init_fn=data.worker_init_fn)\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     ds, batch_size=1, num_workers=8, shuffle=True,\n",
    "#     collate_fn=data.collate_remove_none,\n",
    "#     worker_init_fn=data.worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af74c7bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cfa6abcb82ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata_vis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_vis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcategory_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n/a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'idx'"
     ]
    }
   ],
   "source": [
    "# For visualizations\n",
    "vis_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=False,\n",
    "    collate_fn=data.collate_remove_none,\n",
    "    worker_init_fn=data.worker_init_fn)\n",
    "model_counter = defaultdict(int)\n",
    "data_vis_list = []\n",
    "\n",
    "# Build a data dictionary for visualization\n",
    "iterator = iter(vis_loader)\n",
    "for i in range(len(vis_loader)):\n",
    "    data_vis = next(iterator)\n",
    "    idx = data_vis['idx'].item()\n",
    "    model_dict = val_dataset.get_model_dict(idx)\n",
    "    category_id = model_dict.get('category', 'n/a')\n",
    "    category_name = val_dataset.metadata[category_id].get('name', 'n/a')\n",
    "    category_name = category_name.split(',')[0]\n",
    "    if category_name == 'n/a':\n",
    "        category_name = category_id\n",
    "\n",
    "    c_it = model_counter[category_id]\n",
    "    if c_it < vis_n_outputs:\n",
    "        data_vis_list.append({'category': category_name, 'it': c_it, 'data': data_vis})\n",
    "\n",
    "    model_counter[category_id] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf4b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = config.get_model(cfg, device=device, dataset=train_dataset)\n",
    "\n",
    "# Generator\n",
    "generator = config.get_generator(model, cfg, device=device)\n",
    "\n",
    "# Intialize training\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "trainer = config.get_trainer(model, optimizer, cfg, device=device)\n",
    "\n",
    "checkpoint_io = CheckpointIO(out_dir, model=model, optimizer=optimizer)\n",
    "# try:\n",
    "#     load_dict = checkpoint_io.load('model.pt')\n",
    "# except FileExistsError:\n",
    "#     load_dict = dict()\n",
    "# epoch_it = load_dict.get('epoch_it', 0)\n",
    "# it = load_dict.get('it', 0)\n",
    "# metric_val_best = load_dict.get(\n",
    "#      'loss_val_best', -model_selection_sign * np.inf)\n",
    "\n",
    "# if metric_val_best == np.inf or metric_val_best == -np.inf:\n",
    "#     metric_val_best = -model_selection_sign * np.inf\n",
    "# print('Current best validation metric (%s): %.8f'\n",
    "#       % (model_selection_metric, metric_val_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa0f2ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1978275\n",
      "output path:  out/pointcloud/grid\n"
     ]
    }
   ],
   "source": [
    "# Shorthands\n",
    "print_every = cfg['training']['print_every']\n",
    "checkpoint_every = cfg['training']['checkpoint_every']\n",
    "validate_every = cfg['training']['validate_every']\n",
    "visualize_every = cfg['training']['visualize_every']\n",
    "\n",
    "# Print model\n",
    "nparameters = sum(p.numel() for p in model.parameters())\n",
    "print('Total number of parameters: %d' % nparameters)\n",
    "\n",
    "print('output path: ', cfg['training']['out_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fc0b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_loader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b5e936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nizar/miniconda3/envs/conv_onet/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448272031/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "c = model.encode_inputs(batch.get('inputs').cuda())\n",
    "results = model.decode(batch.get('points').cuda(), c).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418a568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05e2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bc831ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 100] it=100, loss=2.7975, time: 147.94s, 19:05\n",
      "[Epoch 200] it=200, loss=1.6242, time: 218.73s, 19:06\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 300] it=300, loss=1.1940, time: 290.05s, 19:07\n",
      "[Epoch 400] it=400, loss=1.0181, time: 361.30s, 19:09\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 500] it=500, loss=0.8496, time: 450.09s, 19:10\n",
      "[Epoch 600] it=600, loss=0.7280, time: 568.99s, 19:12\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 700] it=700, loss=0.6575, time: 677.83s, 19:14\n",
      "[Epoch 800] it=800, loss=0.5833, time: 773.04s, 19:16\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 900] it=900, loss=0.5414, time: 868.30s, 19:17\n",
      "[Epoch 1000] it=1000, loss=0.4948, time: 963.39s, 19:19\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 1100] it=1100, loss=0.4606, time: 1057.99s, 19:20\n",
      "[Epoch 1200] it=1200, loss=0.4359, time: 1154.45s, 19:22\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 1300] it=1300, loss=0.4164, time: 1251.00s, 19:23\n",
      "[Epoch 1400] it=1400, loss=0.3938, time: 1347.01s, 19:25\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 1500] it=1500, loss=0.3784, time: 1443.48s, 19:27\n",
      "[Epoch 1600] it=1600, loss=0.3603, time: 1561.75s, 19:29\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 1700] it=1700, loss=0.3425, time: 1679.46s, 19:31\n",
      "[Epoch 1800] it=1800, loss=0.3271, time: 1799.67s, 19:33\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 1900] it=1900, loss=0.3135, time: 1919.56s, 19:35\n",
      "[Epoch 2000] it=2000, loss=0.3007, time: 2037.86s, 19:37\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 2100] it=2100, loss=0.2850, time: 2157.77s, 19:39\n",
      "[Epoch 2200] it=2200, loss=0.2717, time: 2276.69s, 19:41\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 2300] it=2300, loss=0.2668, time: 2394.19s, 19:43\n",
      "[Epoch 2400] it=2400, loss=0.2506, time: 2510.90s, 19:44\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 2500] it=2500, loss=0.2399, time: 2630.49s, 19:46\n",
      "[Epoch 2600] it=2600, loss=0.2304, time: 2747.98s, 19:48\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 2700] it=2700, loss=0.2249, time: 2866.51s, 19:50\n",
      "[Epoch 2800] it=2800, loss=0.2141, time: 2985.71s, 19:52\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 2900] it=2900, loss=0.2060, time: 3104.52s, 19:54\n",
      "[Epoch 3000] it=3000, loss=0.2015, time: 3222.95s, 19:56\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 3100] it=3100, loss=0.1940, time: 3342.02s, 19:58\n",
      "[Epoch 3200] it=3200, loss=0.1952, time: 3459.62s, 20:00\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 3300] it=3300, loss=0.1844, time: 3577.03s, 20:02\n",
      "[Epoch 3400] it=3400, loss=0.1787, time: 3695.39s, 20:04\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 3500] it=3500, loss=0.1791, time: 3813.64s, 20:06\n",
      "[Epoch 3600] it=3600, loss=0.1708, time: 3930.82s, 20:08\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 3700] it=3700, loss=0.1657, time: 4032.88s, 20:10\n",
      "[Epoch 3800] it=3800, loss=0.1669, time: 4102.07s, 20:11\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 3900] it=3900, loss=0.1590, time: 4172.81s, 20:12\n",
      "[Epoch 4000] it=4000, loss=0.1546, time: 4243.98s, 20:13\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "Backup checkpoint\n",
      "[Epoch 4100] it=4100, loss=0.1532, time: 4315.69s, 20:15\n",
      "[Epoch 4200] it=4200, loss=0.1485, time: 4386.59s, 20:16\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 4300] it=4300, loss=0.1459, time: 4457.10s, 20:17\n",
      "[Epoch 4400] it=4400, loss=0.1430, time: 4534.25s, 20:18\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 4500] it=4500, loss=0.1396, time: 4648.33s, 20:20\n",
      "[Epoch 4600] it=4600, loss=0.1415, time: 4764.55s, 20:22\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 4700] it=4700, loss=0.1350, time: 4883.24s, 20:24\n",
      "[Epoch 4800] it=4800, loss=0.1322, time: 5001.00s, 20:26\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 4900] it=4900, loss=0.1349, time: 5114.07s, 20:28\n",
      "[Epoch 5000] it=5000, loss=0.1280, time: 5208.79s, 20:29\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 5100] it=5100, loss=0.1255, time: 5304.14s, 20:31\n",
      "[Epoch 5200] it=5200, loss=0.1260, time: 5398.02s, 20:33\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 5300] it=5300, loss=0.1225, time: 5494.01s, 20:34\n",
      "[Epoch 5400] it=5400, loss=0.1199, time: 5590.00s, 20:36\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 5500] it=5500, loss=0.1208, time: 5683.92s, 20:37\n",
      "[Epoch 5600] it=5600, loss=0.1168, time: 5779.53s, 20:39\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 5700] it=5700, loss=0.1145, time: 5875.04s, 20:41\n",
      "[Epoch 5800] it=5800, loss=0.1135, time: 5983.33s, 20:42\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 5900] it=5900, loss=0.1109, time: 6101.88s, 20:44\n",
      "[Epoch 6000] it=6000, loss=0.1089, time: 6218.91s, 20:46\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 6100] it=6100, loss=0.1085, time: 6334.95s, 20:48\n",
      "[Epoch 6200] it=6200, loss=0.1058, time: 6453.06s, 20:50\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 6300] it=6300, loss=0.1039, time: 6570.90s, 20:52\n",
      "[Epoch 6400] it=6400, loss=0.1032, time: 6686.84s, 20:54\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 6500] it=6500, loss=0.1011, time: 6804.90s, 20:56\n",
      "[Epoch 6600] it=6600, loss=0.1028, time: 6922.82s, 20:58\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 6700] it=6700, loss=0.0983, time: 7039.79s, 21:00\n",
      "[Epoch 6800] it=6800, loss=0.0965, time: 7156.19s, 21:02\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 6900] it=6900, loss=0.0981, time: 7272.27s, 21:04\n",
      "[Epoch 7000] it=7000, loss=0.0942, time: 7387.67s, 21:06\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 7100] it=7100, loss=0.0988, time: 7504.95s, 21:08\n",
      "[Epoch 7200] it=7200, loss=0.0920, time: 7623.08s, 21:10\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 7300] it=7300, loss=0.0907, time: 7740.31s, 21:12\n",
      "[Epoch 7400] it=7400, loss=0.0903, time: 7857.14s, 21:14\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 7500] it=7500, loss=0.0902, time: 7974.99s, 21:16\n",
      "[Epoch 7600] it=7600, loss=0.0870, time: 8089.67s, 21:17\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 7700] it=7700, loss=0.0866, time: 8160.63s, 21:19\n",
      "[Epoch 7800] it=7800, loss=0.0870, time: 8231.29s, 21:20\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 7900] it=7900, loss=0.0844, time: 8302.46s, 21:21\n",
      "[Epoch 8000] it=8000, loss=0.0875, time: 8372.43s, 21:22\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "Backup checkpoint\n",
      "[Epoch 8100] it=8100, loss=0.0828, time: 8443.63s, 21:23\n",
      "[Epoch 8200] it=8200, loss=0.0818, time: 8514.57s, 21:25\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 8300] it=8300, loss=0.0807, time: 8585.10s, 21:26\n",
      "[Epoch 8400] it=8400, loss=0.0797, time: 8690.35s, 21:27\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 8500] it=8500, loss=0.0849, time: 8808.74s, 21:29\n",
      "[Epoch 8600] it=8600, loss=0.0788, time: 8927.62s, 21:31\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 8700] it=8700, loss=0.0778, time: 9043.99s, 21:33\n",
      "[Epoch 8800] it=8800, loss=0.0767, time: 9162.83s, 21:35\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 8900] it=8900, loss=0.0766, time: 9281.24s, 21:37\n",
      "[Epoch 9000] it=9000, loss=0.0783, time: 9398.07s, 21:39\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 9100] it=9100, loss=0.0750, time: 9515.92s, 21:41\n",
      "[Epoch 9200] it=9200, loss=0.0761, time: 9617.19s, 21:43\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 9300] it=9300, loss=0.0739, time: 9711.97s, 21:44\n",
      "[Epoch 9400] it=9400, loss=0.0738, time: 9807.93s, 21:46\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 9500] it=9500, loss=0.0726, time: 9904.00s, 21:48\n",
      "[Epoch 9600] it=9600, loss=0.0721, time: 9997.68s, 21:49\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 9700] it=9700, loss=0.0719, time: 10093.05s, 21:51\n",
      "[Epoch 9800] it=9800, loss=0.0711, time: 10189.51s, 21:52\n",
      "Visualizing\n",
      "Saving checkpoint\n",
      "[Epoch 9900] it=9900, loss=0.0755, time: 10283.78s, 21:54\n",
      "[Epoch 10000] it=10000, loss=0.0698, time: 10384.20s, 21:56\n",
      "Visualizing\n",
      "Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "it = 0\n",
    "epoch_it = 0\n",
    "batch = next(train_loader.__iter__())\n",
    "while epoch_it<5000:\n",
    "    epoch_it += 1\n",
    "\n",
    "    it += 1\n",
    "    loss = trainer.train_step(batch)\n",
    "    #logger.add_scalar('train/loss', loss, it)\n",
    "    writer.add_scalar(\"basicModel\", loss, it)\n",
    "\n",
    "    # Print output\n",
    "    if print_every > 0 and (it % print_every) == 0:\n",
    "        t = datetime.datetime.now()\n",
    "        print('[Epoch %02d] it=%03d, loss=%.4f, time: %.2fs, %02d:%02d'\n",
    "                    % (epoch_it, it, loss, time.time() - t0, t.hour, t.minute))\n",
    "\n",
    "    #data_v = next(in data_vis_.__iter__())ist:\n",
    "    # Visualize output\n",
    "    if visualize_every > 0 and (it % visualize_every) == 0:\n",
    "        print('Visualizing')\n",
    "        for data_vis in data_vis_list:\n",
    "            if cfg['generation']['sliding_window']:\n",
    "                out = generator.generate_mesh_sliding(data_vis['data'])\n",
    "            else:\n",
    "                out = generator.generate_mesh(data_vis['data'])\n",
    "            # Get statistics\n",
    "            try:\n",
    "                mesh, stats_dict = out\n",
    "            except TypeError:\n",
    "                mesh, stats_dict = out, {}\n",
    "\n",
    "            mesh.export(os.path.join(out_dir, 'vis', '{}_{}_{}.off'.format(it, data_vis['category'], data_vis['it'])))\n",
    "\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (checkpoint_every > 0 and (it % checkpoint_every) == 0):\n",
    "        print('Saving checkpoint')\n",
    "        checkpoint_io.save('basicModel.pt', epoch_it=epoch_it, it=it)\n",
    "\n",
    "    # Backup if necessary\n",
    "    if (backup_every > 0 and (it % backup_every) == 0):\n",
    "        print('Backup checkpoint')\n",
    "        checkpoint_io.save('basicModel_%d.pt' % it, epoch_it=epoch_it, it=it)\n",
    "        \n",
    "    # Run validation\n",
    "#     if validate_every > 0 and (it % validate_every) == 0:\n",
    "#         eval_dict = trainer.evaluate(val_loader)\n",
    "#         metric_val = eval_dict[model_selection_metric]\n",
    "#         print('Validation metric (%s): %.4f'\n",
    "#                 % (model_selection_metric, metric_val))\n",
    "\n",
    "#         for k, v in eval_dict.items():\n",
    "#             logger.add_scalar('val/%s' % k, v, it)\n",
    "\n",
    "#         if model_selection_sign * (metric_val - metric_val_best) > 0:\n",
    "#             metric_val_best = metric_val\n",
    "#             print('New best model (loss %.4f)' % metric_val_best)\n",
    "#             checkpoint_io.save('model_best.pt', epoch_it=epoch_it, it=it,\n",
    "#                                 loss_val_best=metric_val_best)\n",
    "\n",
    "    # Exit if necessary\n",
    "#     if exit_after > 0 and (time.time() - t0) >= exit_after:\n",
    "#         print('Time limit reached. Exiting.')\n",
    "#         checkpoint_io.save('model.pt', epoch_it=epoch_it, it=it,\n",
    "#                             loss_val_best=metric_val_best)\n",
    "\n",
    "#print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4ef18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conv_onet",
   "language": "python",
   "name": "conv_onet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
